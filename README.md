# ğŸ” Deteksi Anomali Log Sistem Menggunakan Model Sequence Berbasis Deep Learning

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.10%2B-orange.svg)](https://tensorflow.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rehanalfarizu/big-data-mining-/blob/main/Anomaly_Detection_Colab.ipynb)

## ğŸ“‹ Deskripsi Proyek

Proyek ini mengimplementasikan sistem deteksi anomali pada log sistem menggunakan model deep learning berbasis sequence (LSTM, GRU, Bi-LSTM, dan **Transformer**). Sistem ini mampu mengidentifikasi pola abnormal dalam log sistem yang dapat mengindikasikan serangan keamanan, kegagalan sistem, atau aktivitas mencurigakan lainnya.

### âœ¨ Fitur Utama

- **Multiple Model Architectures**: LSTM, GRU, Bi-LSTM, CNN-LSTM, Autoencoder, dan **Transformer**
- **Transfer Learning**: Support untuk pre-trained embeddings (GloVe, FastText, Word2Vec)
- **Public Datasets**: Integrasi dengan dataset publik (HDFS, BGL, Thunderbird)
- **Attention Mechanism**: Self-attention dan Multi-head attention
- **Comprehensive Evaluation**: ROC-AUC, Precision-Recall, Confusion Matrix

## ğŸ¯ Tujuan

1. **Deteksi Otomatis**: Mendeteksi anomali secara otomatis tanpa memerlukan aturan manual
2. **Real-time Processing**: Mampu memproses log secara real-time
3. **Sequence Learning**: Memahami pola sekuensial dalam log sistem
4. **Skalabilitas**: Dapat diterapkan pada berbagai jenis log sistem
5. **Transfer Learning**: Memanfaatkan pre-trained embeddings untuk performa lebih baik

## ğŸ—ï¸ Arsitektur Sistem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw System    â”‚â”€â”€â”€â”€â–¶â”‚  Preprocessing  â”‚â”€â”€â”€â”€â–¶â”‚    Tokenizer    â”‚
â”‚      Logs       â”‚     â”‚    & Parsing    â”‚     â”‚   & Embedding   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                               â”‚                               â”‚
                        â–¼                               â–¼                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   LSTM / GRU    â”‚            â”‚   Transformer   â”‚            â”‚   CNN-LSTM      â”‚
              â”‚   Bi-LSTM       â”‚            â”‚   + Attention   â”‚            â”‚    Hybrid       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                               â”‚                               â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚    Anomaly      â”‚
                                              â”‚   Detection     â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Struktur Proyek

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Data log mentah
â”‚   â”œâ”€â”€ processed/              # Data yang sudah diproses
â”‚   â”œâ”€â”€ public/                 # Dataset publik (HDFS, BGL, Thunderbird)
â”‚   â””â”€â”€ sample_logs.txt         # Contoh data log
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_generator.py       # Generator data log sintetis
â”‚   â”œâ”€â”€ preprocessing.py        # Module preprocessing
â”‚   â”œâ”€â”€ tokenizer.py           # Tokenizer untuk log
â”‚   â”œâ”€â”€ model.py               # Definisi model LSTM/GRU
â”‚   â”œâ”€â”€ transformer_model.py   # ğŸ†• Model Transformer
â”‚   â”œâ”€â”€ transfer_learning.py   # ğŸ†• Pre-trained embeddings
â”‚   â”œâ”€â”€ public_dataset_loader.py # ğŸ†• Loader dataset publik
â”‚   â”œâ”€â”€ train.py               # Script training
â”‚   â””â”€â”€ inference.py           # Script inferensi/prediksi
â”œâ”€â”€ models/
â”‚   â””â”€â”€ saved_models/          # Model yang sudah ditraining
â”œâ”€â”€ embeddings/                 # ğŸ†• Pre-trained embeddings cache
â”‚   â”œâ”€â”€ pretrained/
â”‚   â””â”€â”€ custom/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ anomaly_detection_demo.ipynb  # Notebook demonstrasi
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml            # Konfigurasi proyek
â”œâ”€â”€ results/
â”‚   â””â”€â”€ visualizations/        # Hasil visualisasi
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # Dokumentasi
â””â”€â”€ LICENSE                    # Lisensi MIT
```

## ğŸš€ Instalasi

### 1. Clone Repository
```bash
git clone https://github.com/username/Deteksi-Anomali-Log-Sistem.git
cd Deteksi-Anomali-Log-Sistem
```

### 2. Buat Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# atau
venv\Scripts\activate     # Windows
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

## ğŸ“Š Dataset

### Dataset Publik yang Didukung

| Dataset | Deskripsi | Jumlah Log | Source |
|---------|-----------|------------|--------|
| **HDFS** | Hadoop Distributed File System logs | 11M+ logs | [LogHub](https://github.com/logpai/loghub) |
| **BGL** | Blue Gene/L supercomputer logs | 4.7M logs | [LogHub](https://github.com/logpai/loghub) |
| **Thunderbird** | Thunderbird supercomputer logs | 211M logs | [LogHub](https://github.com/logpai/loghub) |

### Load Dataset Publik

```python
from src.public_dataset_loader import load_public_dataset

# Load HDFS dataset
df, info = load_public_dataset('hdfs', max_logs=10000)

# Load BGL dataset
df, info = load_public_dataset('bgl', max_logs=10000)
```

### Format Log yang Didukung

1. **Syslog Format**
```
Jan  4 10:15:23 server01 sshd[12345]: Failed password for invalid user admin from 192.168.1.100 port 22 ssh2
```

2. **Apache Access Log**
```
192.168.1.100 - - [04/Jan/2026:10:15:23 +0700] "GET /admin HTTP/1.1" 404 512
```

3. **Custom Log Format**
```
2026-01-04 10:15:23 ERROR [ModuleName] Error message description
```

### Generate Data Sintetis

```bash
python src/data_generator.py --num_logs 10000 --anomaly_ratio 0.1 --output data/raw/
```

## ğŸ§  Model Deep Learning

### Model yang Tersedia

| Model | Deskripsi | Parameter |
|-------|-----------|-----------|
| **LSTM** | Long Short-Term Memory | ~150K |
| **GRU** | Gated Recurrent Unit | ~120K |
| **Bi-LSTM** | Bidirectional LSTM dengan Attention | ~300K |
| **CNN-LSTM** | Hybrid CNN + LSTM | ~200K |
| **Autoencoder** | LSTM Autoencoder | ~250K |
| **Transformer** | ğŸ†• Multi-head Self-Attention | ~400K |
| **CNN-Transformer** | ğŸ†• Hybrid CNN + Transformer | ~350K |

### Arsitektur Transformer

```
Input Layer (sequence_length)
           â”‚
           â–¼
    Token + Positional Embedding
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Transformer Block x N  â”‚
    â”‚  â”œâ”€ Multi-Head Attention â”‚
    â”‚  â”œâ”€ Add & Norm           â”‚
    â”‚  â”œâ”€ Feed Forward         â”‚
    â”‚  â””â”€ Add & Norm           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    Global Average Pooling
           â”‚
           â–¼
    Dense Layer (units=64, activation='relu')
           â”‚
           â–¼
    Output Layer (units=1, activation='sigmoid')
```

### Arsitektur LSTM/GRU

```
Input Layer (sequence_length, vocab_size)
           â”‚
           â–¼
    Embedding Layer (embedding_dim=128)
           â”‚
           â–¼
    LSTM/GRU Layer (units=64, return_sequences=True)
           â”‚
           â–¼
    Dropout Layer (rate=0.3)
           â”‚
           â–¼
    LSTM/GRU Layer (units=32)
           â”‚
           â–¼
    Dense Layer (units=64, activation='relu')
           â”‚
           â–¼
    Output Layer (units=1, activation='sigmoid')
```

### Parameter Model

| Parameter | Nilai Default | Deskripsi |
|-----------|---------------|-----------|
| sequence_length | 50 | Panjang sequence input |
| embedding_dim | 128 | Dimensi embedding |
| lstm_units | [64, 32] | Unit LSTM per layer |
| num_heads | 4 | Attention heads (Transformer) |
| ff_dim | 256 | Feed-forward dimension |
| dropout_rate | 0.3 | Dropout rate |
| learning_rate | 0.001 | Learning rate |
| batch_size | 32 | Batch size |
| epochs | 50 | Jumlah epoch |

## ğŸ”„ Transfer Learning

### Pre-trained Embeddings yang Didukung

| Embedding | Dimensi | Source |
|-----------|---------|--------|
| **GloVe** | 50, 100, 200, 300 | Stanford NLP |
| **FastText** | 100, 300 | Facebook AI |
| **Word2Vec** | 100, 300 | Google |
| **Custom** | Configurable | Train on logs |

### Penggunaan Transfer Learning

```python
from src.transfer_learning import PretrainedEmbeddingLoader

# Load pre-trained embeddings
loader = PretrainedEmbeddingLoader()
embeddings = loader.load_glove(dim=100)

# Create embedding matrix
embedding_matrix = loader.get_embedding_matrix(tokenizer.word_index)
```

## ğŸ’» Penggunaan

### Training Model

```bash
# Training dengan LSTM (default)
python src/train.py --config config/config.yaml

# Training dengan Transformer
python src/train.py --config config/config.yaml --model transformer

# Training dengan transfer learning
python src/train.py --config config/config.yaml --use-pretrained-embeddings
```

### Contoh Kode Training Transformer

```python
from src.transformer_model import create_transformer_model

# Create model
model = create_transformer_model(
    model_type='transformer',
    vocab_size=10000,
    max_length=50,
    embed_dim=128,
    num_heads=4,
    num_blocks=2
)

# Compile
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Train
model.fit(X_train, y_train, epochs=50, validation_split=0.15)
```

### Inferensi/Prediksi

```bash
python src/inference.py --model models/saved_models/best_model.keras --input data/test_logs.txt
```

### Jupyter Notebook

```bash
jupyter notebook notebooks/anomaly_detection_demo.ipynb
```

## ğŸ“ˆ Hasil Eksperimen

### Metrik Evaluasi

| Model | Accuracy | Precision | Recall | F1-Score | AUC-ROC |
|-------|----------|-----------|--------|----------|---------|
| LSTM | 95.2% | 93.8% | 94.5% | 94.1% | 0.97 |
| GRU | 94.8% | 93.2% | 94.0% | 93.6% | 0.96 |
| Bi-LSTM | 96.1% | 94.5% | 95.2% | 94.8% | 0.98 |
| **Transformer** | **96.8%** | **95.2%** | **95.8%** | **95.5%** | **0.98** |
| CNN-Transformer | 96.5% | 94.8% | 95.5% | 95.1% | 0.98 |

### Confusion Matrix

```
              Predicted
            Normal  Anomaly
Actual  Normal   4521    124
       Anomaly    89    1266
```

## ğŸ”¬ Metodologi

### 1. Preprocessing
- Parsing log messages
- Tokenisasi teks
- Normalisasi timestamp, IP, path
- Encoding kategorikal

### 2. Feature Engineering
- Token embeddings
- Positional encodings
- Pre-trained word embeddings (Transfer Learning)
- Temporal features

### 3. Model Training
- Train/Validation/Test split (70/15/15)
- Early stopping
- Model checkpointing
- Learning rate scheduling
- Mixed precision training (opsional)

### 4. Evaluasi
- Cross-validation
- ROC-AUC analysis
- Precision-Recall curves
- Confusion matrix

## ğŸ›¡ï¸ Jenis Anomali yang Dideteksi

1. **Brute Force Attack**
   - Multiple failed login attempts
   - Password spraying

2. **Privilege Escalation**
   - Unauthorized sudo access
   - Permission changes

3. **Suspicious Network Activity**
   - Unusual connection patterns
   - Data exfiltration indicators

4. **System Errors**
   - Service failures
   - Resource exhaustion

5. **Malware Indicators**
   - Suspicious process execution
   - File system anomalies

## ğŸ“š Referensi

1. Du, M., et al. (2017). "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning"
2. Hochreiter, S., & Schmidhuber, J. (1997). "Long Short-Term Memory"
3. Cho, K., et al. (2014). "Learning Phrase Representations using RNN Encoder-Decoder"
4. **Vaswani, A., et al. (2017). "Attention Is All You Need" - NeurIPS**
5. Devlin, J., et al. (2018). "BERT: Pre-training of Deep Bidirectional Transformers"
6. He, P., et al. (2016). "An Evaluation Study on Log Parsing"

## ğŸ“¦ Dependencies

```
tensorflow>=2.10.0
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
pyyaml>=6.0
tqdm>=4.62.0
gensim>=4.0.0  # Untuk Word2Vec training
requests>=2.26.0  # Untuk download dataset
```

## ğŸ‘¨â€ğŸ’» Kontributor

- **Nama Mahasiswa** - *Initial work* - [GitHub Profile](https://github.com/username)

## ğŸ“„ Lisensi

Proyek ini dilisensikan di bawah MIT License - lihat file [LICENSE](LICENSE) untuk detail.

## ğŸ™ Acknowledgments

- TensorFlow Team
- Keras Team
- LogHub (Dataset Provider)
- Stanford NLP (GloVe)
- Komunitas Deep Learning Indonesia

---

â­ Jika proyek ini bermanfaat, silakan berikan star!

ğŸ“§ Untuk pertanyaan: email@domain.com
